<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TeXtidote analysis</title>
<style type="text/css">
body {
  font-family: sans-serif;
}
.highlight, .highlight-sh, .highlight-spelling {
  padding: 2pt;
  border-radius: 4pt;
  cursor: help;
  opacity: 0.7;
  border: dashed 1px;
}
.highlight {
  background-color: orange;
  color: black;
}
.highlight-sh {
  background-color: yellow;
  color: black;
}
.highlight-spelling {
  background-color: red;
  color: white;
}
div.original-file {
  font-family: monospace;
  font-size: 11pt;
  background-color: #f8f8ff;
  padding: 20pt;
  border-radius: 6pt;
}
.textidote {
  	background-image: url(data:image/svg+xml;base64,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);
}
h2.filename {
  font-family: monospace;
}
h1.textidote {
  width: 378px;
  height: 68px;
  display: block;
}
.keyword1 {
  font-weight: bold;
  color: green;
}
.keyword2 {
  font-weight: bold;
  color: darkblue;
}
.comment, .comment * {
  color: darkred;
  font-weight: normal;
}
.linenb {
  font-style: italic;
  color: lightgrey;
  width: 30pt;
  float: left;
  margin-top: 1pt;
  margin-bottom: 1pt;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.codeline {
  margin-left: -30pt;
  padding-left: 60pt;
  margin-top: 1pt;
  margin-bottom: 1pt;
}
.no-text {
  display: none;
}
.clear {
  clear: both;
}
</style>
</head>
<body>
<a href="https://sylvainhalle.github.io/textidote"><h1 class="textidote"><span class="no-text">Results of TeXtidote analysis</span></h1></a>
<p>Here is the result of analyzing your file(s) with TeXtidote. Hover the mouse over highlighted portions of the document to read a tooltip that gives you some writing advice.</p>
<p>Found 7 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline"><span class="comment"><span class="comment">% Chapter Template</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 111 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span>{Proposed Methodology II} <span class="comment">% Main chapter title</span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline"><span class="keyword1">\label</span>{chp:proposed2} <span class="comment">% Change X to a consecutive number; for referencing this chapter elsewhere, use~\ref{ChapterX}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">CNN, like many computer vision models, is a scale-variant~\cite{van2017learning} model such that it cannot recognize objects at various scales unless it explicitly trained to recognize such objects. Data augmentation can accomplish some degree of invariance as it allows the network to be trained with distorted samples, but it not the case for pneumonia scales. This chapter presents a CNN architecture that learns multiscale features using scale pyramid of the  CNN's internal feature maps. Scale pyramid is constructed using atrous convolution of various dilation rates. The correct scale from scale pyramid that allows minimization of the objective function loss is selected using the spatial attention mechanism. </div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline"><span class="keyword1">\section</span>{Methodology II} </div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline"><span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">    <span class="keyword2">\begin{figure*}</span>[htbp]</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">    \centerline{<span class="keyword1">\includegraphics</span>[height=40mm,width=15cm]{Figures/ProposedPipe.png}}</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline"><span class="highlight-sh" title="This figure is missing a label [sh:figref]"> </span>   <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{Proposed method for COVID-19 classification from CXR images.}<span class="keyword1">\label</span>{ProposedPipe</span>}<span class="keyword2">\end{figure*}</span><span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">    </div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline">The proposed system presented in this chapter proposes a novel CNN micro-architecture model for learning scale-invariant features from row input CXR images and then classifies these features into normal or COVID-19 cases. Fig.~\ref{ProposedPipe} illustrates trainable end-to-end pipeline of the proposed system. The proposed system depends on a novel Spatially weighted Atrous Spatial Pyramid Pooling (SWASPP) to extract multi-scale features of input CXR images. A novel attention module is then used to fuse the extracted these multi-scale features and select relevant features' scale that the next layer should consider.</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline"><span class="keyword1">\subsection</span>{Data augmentation}</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline"><span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">    <span class="keyword2">\begin{figure}</span>[htbp]</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">    \centerline{<span class="keyword1">\includegraphics</span>[height=30mm,width=9cm]{Figures/TexAug.PNG}}</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">    <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{Texture Augmentation module</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">    <span class="keyword1">\label</span>{texaug}</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">    <span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">    <span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">The first phase of the proposed CXR classification system is data augmentation phase. Data augmentation is used to reduce the overfitting by artificially enlarge the training dataset~\cite{krizhevsky2012imagenet} using label preserving transformation. Data augmentation phase introduces a degree invariance to a distortion transformation such as the flipping and rotation. The input CXR images are augmented using texture augmentation.  Texture augmentation is performed by introducing a multiplicative normally distributed noises to the frequency spectrum of the input image. CXR image is transformed to the frequency spectrum using the fourier transform.  Noise is modeled using $\mathcal{N}(\mu = 1,\,\sigma = 0.3)$. Fig.~\ref{texaug} illustrates texture augmentation process for frequency distortion of the CXR image. Fig.~\ref{resltaug} shows the original CXR image and the corresponding frequency distorted CXR image. A standard augmentation techniques such as random rotation, horizontal flipping, and vertical flipping are included in the augmentation process. </div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline"><span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">    <span class="keyword2">\begin{figure}</span>[htbp]</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">    \centerline{<span class="keyword1">\includegraphics</span>[height=40mm,width=9cm]{Figures/freqJitt.png}}</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">    <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{Texture Augmentation</span>}{The resulting CXR image from Texture augmentation <span class="keyword1">\textbf</span>{left}: is the original image. <span class="keyword1">\textbf</span>{Right} is the augmented  CXR Image}</div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">    <span class="keyword1">\label</span>{resltaug}</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">    <span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">    <span class="keyword2">\end{center}</span> </div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">    </div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline"><span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[htbp]</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">\centerline{<span class="keyword1">\includegraphics</span>[height=50mm,width=9cm]{Figures/SWASPP.PNG}}</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline"><span class="keyword1">\caption</span>{Spatially weighted atrous spatial Pyramid Pooling (SWASPP) interal layers within dashed square are parameter shared.}</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline"><span class="keyword1">\label</span>{swaspp}</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline"><span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline"><span class="keyword1">\subsection</span>{Spatially Weighted Atrous Spatial Pyramid Pooling}</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">Atrous convolution is a powerful technique for adjusting the resolution of convolutional kernels. This allows to effectively enlarge the field-of-view of the kernel without increasing neither the number of kernel parameters nor the computational complexity of the convolution operation. Atrous convolution is equivalent to performing downsampling and then performing convolution with original kernel without dilation. As a result different dilation rates of the kernel corresponding to different downsampling degrees. A novel spatially weighted atrous spatial pyramid pooling (SWASPP) micro-architecture is presented that exploit the scale space of the CNN's feature maps. Fig.~\ref{swaspp} shows the architecture of the SWASPP. In Fig.~\ref{swaspp}, internal pipelines, bounded by dashed-line square, are parameter-shared and each pipeline of these has a different dilation rates. These pipelines are responsible for extracting multi-scale, scale invariant, features. Sharing of the parameters enforce these pipelines to learn features that exists at multiple levels of scale-pyramid and hence scale-invariance. For a given input CXR image, three scales feature maps are produced.</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline"><span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[htbp]</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">\centerline{<span class="keyword1">\includegraphics</span>[height=60mm,width=3.5cm]{Figures/AttentionModUl.PNG}}</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline"><span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{Attantion module structure used by SWASPP micro-architecture</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline"><span class="keyword1">\label</span>{attain}</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline"><span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">To fuse these feature maps produced by different pipeline of the SWASPP from the input feature map, an attention module is emerged. Attention module can be thought as a pixel level classification of which scale does this pixel it belongs to. Fig.~\ref{attain} illustrates the proposed attention module structure. Proposed attention module generates four heatmaps. The first three heatmaps correspond to the three scale feature maps while the remaining heatmap corresponds to the input feature map itself. These heatmaps are summed  up to one (<span class="keyword1">\textit</span>{i.e.,} for a  spatial position </div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">$(x, y)$, $\sum_{i =1}^{4} H(i,x,y) = 1$ where $H(i,x,y)$ is the $i$ heatmap produced by the attention module). To make sure this property holds, softmax function is used. </div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">The proposed mirco-architecture uses a pixel level weights produced by corresponding attention module rather than a single weight value for each scale. A single input CXR image may have multiple COVID-19 pneumonia scales which effectively lead to simply averaging the scale space when using single weight for each scale on scale space. In SWASPP, every convolution operation is followed by a BN and leakyReLU~\cite{krizhevsky2012imagenet} non-linearity except the re-projection layers that used to project back to the input space is not followed by nonlinearity. </div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">BN allows the use of larger learning rate~\cite{ioffe2015batch} and makes network stable during training~\cite{ioffe2015batch}. BN makes the loss landscape of the optimization problem significantly smoother~\cite{santurkar2018does}.</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">leakyReLU is used to reduce the vanishing gradient problem~\cite{krizhevsky2012imagenet}.</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">A bottleneck is introduced within both the attention module and multi-scale feature extractor pipeline. A bottleneck in SWASPP is used to project the input feature map of dimension $C_{in}\times H\times W$ to $32\times H\times W$ then re-project back to $C_{in}\times H\times W$. Multi-scale feature extraction is preformed on the projected dimension. Same logic is applied to the attention module where the input feature map is projected to a dimension of $16\times H\times W$.</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">This bottleneck allows the efficient use of model capacity and reduce the network computational complexity~\cite{huang2017densely}. It only allows the flow of important information and discarding irrelevant information. </div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline"><span class="keyword1">\subsection</span>{Proposed CNN Architecture}</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">SWASPP is densely stacked~\cite{huang2017densely} together as Fig.~\ref{denseB} illustrates. This kind of connectivity allows implicit deep supervisions as each layer is effectively connected to the last layer using shorter path also facilitate feature reuse~\cite{huang2017densely} and gradient flow. Residual layers are easier to optimize if the required mapping is the identity mapping or simply near to it~\cite{he2016deep}. Densely stacked SWASPP is denoted by (DSWASPP). Convolutional part of proposed model consists of stacking six DSWASPP layers such that the first four layers are interconnected using maxpooling to reduce the spatial size and enlarge the Network receptive field. A single level Spatial Pyramid Pooling (SPP)~\cite{he2015spatial} is added after to produce a fixed size feature vector for a variable size input. SPP layer divides the input feature map into $10\times 10 = 100$ bins then performs a $max$ for each bin as an aggregation function. </div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline"><span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[htbp]</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">\centerline{<span class="keyword1">\includegraphics</span>[height=30mm,width=6cm]{Figures/DensResd.PNG}}</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline"><span class="keyword1">\caption</span>{Densely connected SWASPP (DSWASPP): is a stack of densely connected SWASPP, such that the output of any SWASPP is Concatenated to the input of all next layers. All the three layers produce an output of dimension of $C_{in} \times H \times W$.}</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline"><span class="keyword1">\label</span>{denseB}</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline"><span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">The fixed length feature vector produced by SPP is used as an input to dropout~\cite{srivastava2014dropout} layer. Dropout layer randomly sets the activation of to $0$ with a probability of $0.5$. Dropout prevents the overfitting and reduce complex co-adaptation between the neurons allowing them to learn better representation~\cite{srivastava2014dropout}. It allow implicit ensempling of exponential number of sampled thin network from the original network which enhance the network performance~\cite{srivastava2014dropout}. The result of dropout layer is used as input to the classification network. Classification network consists of a fully connected layers with a $3$ Dense layers such that the output layer is 2-neuron for binary classification <span class="keyword1">\textit</span>{i.e)} COVID19 or not. Table~\ref{PCNN} shows the details of the proposed architecture.</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">\renewcommand{\arraystretch}{1.5}</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline"><span class="keyword2">\begin{table}</span>[htbp]</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">    <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{Proposed CNN architecture of methodology II</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">    <span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">    <span class="keyword2">\begin{tabular}</span>{|c|c|c|c|}</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">    <span class="keyword1">\textbf</span>{Layer}&amp;\multicolumn{3}{|c|}{<span class="keyword1">\textbf</span>{Proposed CNN Architecture of Methodology II}} \\</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">    \cline{2-4} </div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">    <span class="keyword1">\textbf</span>{Name} &amp; <span class="keyword1">\textbf</span>{<span class="keyword1">\textit</span>{Input Shape}}&amp; <span class="keyword1">\textbf</span>{<span class="keyword1">\textit</span>{Output Shape}}&amp; <span class="keyword1">\textbf</span>{<span class="keyword1">\textit</span>{Param. Count}} \\</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">    Input layer &amp; - &amp; $1 \times 320 \times 320$ &amp; 0 \\</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">    BatchNorm-1 &amp; $1 \times 320 \times 320$ &amp; $1 \times 320 \times 320$ &amp; 2 \\</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">    DSWASPP-1&amp; $1 \times 320 \times 320$ &amp; $32 \times 320 \times 320$ &amp; 121,035  \\</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">    Maxpooling-1&amp; $32 \times 320 \times 320$ &amp;$32 \times 160 \times 160$ &amp; 0 \\</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">    DSWASPP-2&amp; $32 \times 160 \times 160$ &amp; $64 \times 160 \times 160$ &amp; 298,236  \\</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">    Maxpooling-2 &amp; $64 \times 160 \times 160$ &amp; $64 \times 80 \times 80$ &amp;0  \\</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">    DSWASPP-3  &amp; $64 \times 80 \times 80$ &amp; $128 \times 80 \times 80$ &amp; 604,956  \\</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">    Maxpooling-3 &amp; $128 \times 80 \times 80$ &amp; $128 \times 40 \times 40$ &amp; 0  \\</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">    DSWASPP-4  &amp; $128 \times 80 \times 80$ &amp; $128 \times 80 \times 80$ &amp; 784,092 \\</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">    DSWASPP-5  &amp; $128 \times 80 \times 80$ &amp; $128 \times 80 \times 80$ &amp; 784,092 \\</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">    DSWASPP-6  &amp; $128 \times 80 \times 80$ &amp; $128 \times 80 \times 80$ &amp; 784,092 \\</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">    SPP-1 &amp; $128 \times 80 \times 80$ &amp; $12800$ &amp; 0 \\</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">    Dropout-1 &amp; $12800$ &amp; $12800$ &amp; 0 \\</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">    FC-1 &amp; $12800$ &amp; $128$ &amp; 1,638,528 \\</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">    FC-2 &amp; $128$ &amp; $128$ &amp; 16,512 \\</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">    FC-3 &amp; $128$ &amp; $64$ &amp; 8,256 \\</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline">    FC-4 &amp; $64$ &amp; $2$ &amp; 130 \\</div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">    Softmax &amp; $2$ &amp; $2$ &amp; 0 \\</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">    \multicolumn{3}{|c|}{Total Number of Parameter}&amp;5,040,571\\</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">    \hline</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline">    \multicolumn{4}{c}{Any linear combination is followed by BN and leakyReLU nonlinearity}\\</div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">    \multicolumn{4}{l}{excluding re-projection layer of the SWASPP modules}</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline">    <span class="keyword2">\end{tabular}</span></div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline">    <span class="keyword1">\label</span>{PCNN}</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">    <span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">    <span class="keyword2">\end{table}</span></div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline"><span class="keyword1">\section</span>{Summary}</div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline"><span class="comment"><span class="comment">% CNN is a scale variant model. Many approaches were introduced to overcome this problem such as shared networks, feature pyramid network and atrous convolution. Atrous convolution increases the receptive field of the convolutional kernel without neither increasing the parameter number nor the computational complexity. Atrous convolution is used in the proposed work II to construct the scale space of the input feature. Attention mechanism is used to guide to process the most relevant part of the feature maps. To select the correct scale and fuse multiple scales of the input feature map a spatial attention module is used. A novel CNN architecture is proposed that internally produces multiscale feature maps which is further fused using attention based mechanism. Compact representation is learned via a bottleneck dimension which is introduced in both the multiscale feature extractor module and the attention module.</span></span></div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">Convolutional Neural Networks (CNNs) have been widely used in computer vision tasks, including image classification, object detection, and segmentation. However, CNNs are known to be scale variant models, meaning that they can miss important features at different scales. To overcome this issue, various approaches have been proposed, such as shared networks, feature pyramid networks, and atrous convolution.</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">Atrous convolution, also known as dilated convolution, increases the receptive field of the convolutional kernel without increasing the number of parameters or computational complexity. In the proposed work II, atrous convolution is used to construct the scale space of the input feature, allowing the CNN to extract multiscale features.</div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">Moreover, to select the correct scale and fuse multiple scales of the input feature map, a spatial attention module is used in the proposed work II. This attention mechanism guides the network to focus on the most relevant parts of the feature maps, which helps to improve the accuracy of the network.</div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">To further enhance the performance of the network, a novel CNN architecture is proposed in which multiscale feature maps are internally produced and then fused using an attention-based mechanism. This approach leads to a compact representation of the input data via a bottleneck dimension introduced in both the multiscale feature extractor module and the attention module. Overall, these techniques and architectures help to improve the performance and accuracy of CNNs for computer vision tasks.</div><div class="clear"></div>
</div>
<hr/>
Output produced by TeXtidote v0.8.2, &copy; 2018-2020 Sylvain Hall&eacute; - All rights reserved.<br/>
See the <a href="https://sylvainhalle.github.io/textidote">TeXtidote website</a> for more information.
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TeXtidote analysis</title>
<style type="text/css">
body {
  font-family: sans-serif;
}
.highlight, .highlight-sh, .highlight-spelling {
  padding: 2pt;
  border-radius: 4pt;
  cursor: help;
  opacity: 0.7;
  border: dashed 1px;
}
.highlight {
  background-color: orange;
  color: black;
}
.highlight-sh {
  background-color: yellow;
  color: black;
}
.highlight-spelling {
  background-color: red;
  color: white;
}
div.original-file {
  font-family: monospace;
  font-size: 11pt;
  background-color: #f8f8ff;
  padding: 20pt;
  border-radius: 6pt;
}
.textidote {
  	background-image: url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiIHN0YW5kYWxvbmU9Im5vIj8+PHN2ZyAgIHhtbG5zOmRjPSJodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyIgICB4bWxuczpjYz0iaHR0cDovL2NyZWF0aXZlY29tbW9ucy5vcmcvbnMjIiAgIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyIgICB4bWxuczpzdmc9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiAgIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgICB4bWxuczpzb2RpcG9kaT0iaHR0cDovL3NvZGlwb2RpLnNvdXJjZWZvcmdlLm5ldC9EVEQvc29kaXBvZGktMC5kdGQiICAgeG1sbnM6aW5rc2NhcGU9Imh0dHA6Ly93d3cuaW5rc2NhcGUub3JnL25hbWVzcGFjZXMvaW5rc2NhcGUiICAgd2lkdGg9IjEwMC4wOTEwNW1tIiAgIGhlaWdodD0iMTguMjA5MDk5bW0iICAgdmlld0JveD0iMCAwIDEwMC4wOTEwNSAxOC4yMDkwOTkiICAgdmVyc2lvbj0iMS4xIiAgIGlkPSJzdmc4IiAgIGlua3NjYXBlOnZlcnNpb249IjAuOTEgcjEzNzI1IiAgIHNvZGlwb2RpOmRvY25hbWU9InRleHRpZG90ZS5zdmciPiAgPGRlZnMgICAgIGlkPSJkZWZzMiIgLz4gIDxzb2RpcG9kaTpuYW1lZHZpZXcgICAgIGlkPSJiYXNlIiAgICAgcGFnZWNvbG9yPSIjZmZmZmZmIiAgICAgYm9yZGVyY29sb3I9IiM2NjY2NjYiICAgICBib3JkZXJvcGFjaXR5PSIxLjAiICAgICBpbmtzY2FwZTpwYWdlb3BhY2l0eT0iMC4wIiAgICAgaW5rc2NhcGU6cGFnZXNoYWRvdz0iMiIgICAgIGlua3NjYXBlOnpvb209IjEiICAgICBpbmtzY2FwZTpjeD0iLTI1NC4yNTMwOSIgICAgIGlua3NjYXBlOmN5PSItMjc4LjM3NTkxIiAgICAgaW5rc2NhcGU6ZG9jdW1lbnQtdW5pdHM9Im1tIiAgICAgaW5rc2NhcGU6Y3VycmVudC1sYXllcj0ibGF5ZXIxIiAgICAgc2hvd2dyaWQ9ImZhbHNlIiAgICAgZml0LW1hcmdpbi10b3A9IjAiICAgICBmaXQtbWFyZ2luLWxlZnQ9IjAiICAgICBmaXQtbWFyZ2luLXJpZ2h0PSIwIiAgICAgZml0LW1hcmdpbi1ib3R0b209IjAiICAgICBpbmtzY2FwZTp3aW5kb3ctd2lkdGg9IjE5MjAiICAgICBpbmtzY2FwZTp3aW5kb3ctaGVpZ2h0PSIxMDIxIiAgICAgaW5rc2NhcGU6d2luZG93LXg9IjAiICAgICBpbmtzY2FwZTp3aW5kb3cteT0iMjY1IiAgICAgaW5rc2NhcGU6d2luZG93LW1heGltaXplZD0iMSIgLz4gIDxtZXRhZGF0YSAgICAgaWQ9Im1ldGFkYXRhNSI+ICAgIDxyZGY6UkRGPiAgICAgIDxjYzpXb3JrICAgICAgICAgcmRmOmFib3V0PSIiPiAgICAgICAgPGRjOmZvcm1hdD5pbWFnZS9zdmcreG1sPC9kYzpmb3JtYXQ+ICAgICAgICA8ZGM6dHlwZSAgICAgICAgICAgcmRmOnJlc291cmNlPSJodHRwOi8vcHVybC5vcmcvZGMvZGNtaXR5cGUvU3RpbGxJbWFnZSIgLz4gICAgICAgIDxkYzp0aXRsZSAvPiAgICAgIDwvY2M6V29yaz4gICAgPC9yZGY6UkRGPiAgPC9tZXRhZGF0YT4gIDxnICAgICBpbmtzY2FwZTpsYWJlbD0iTGF5ZXIgMSIgICAgIGlua3NjYXBlOmdyb3VwbW9kZT0ibGF5ZXIiICAgICBpZD0ibGF5ZXIxIiAgICAgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTI5LjczODA5NSwtNzAuNTc3NzUxKSI+ICAgIDxnICAgICAgIHN0eWxlPSJmb250LXN0eWxlOm5vcm1hbDtmb250LXdlaWdodDpub3JtYWw7Zm9udC1zaXplOjIwLjkyODk0NTU0cHg7bGluZS1oZWlnaHQ6MS4yNTtmb250LWZhbWlseTpzYW5zLXNlcmlmO2xldHRlci1zcGFjaW5nOjBweDt3b3JkLXNwYWNpbmc6MHB4O2ZpbGw6I2ZmZmZmZjtmaWxsLW9wYWNpdHk6MTtzdHJva2U6IzAwMDAwMDtzdHJva2Utd2lkdGg6MS45Nzc1MzgyMztzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9rZS1kYXNoYXJyYXk6bm9uZSIgICAgICAgaWQ9InRleHQ4MzYiPiAgICAgIDxwYXRoICAgICAgICAgZD0ibSAzMC43MjY4NjQsNzEuNTY2NTIgNS4yMzIyMzYsMCAwLDQuMjA5MDQ0IDUuODEzNTk2LDAgMCwzLjYwNDQyOSAtNS44MTM1OTYsMCAwLDQuODEzNjU4IDUuODEzNTk2LDAgMCwzLjYwNDQyOSAtMTEuMDQ1ODMyLDAgMCwtMTYuMjMxNTYgeiIgICAgICAgICBzdHlsZT0iZm9udC1zdHlsZTpub3JtYWw7Zm9udC12YXJpYW50Om5vcm1hbDtmb250LXdlaWdodDpub3JtYWw7Zm9udC1zdHJldGNoOm5vcm1hbDtmb250LWZhbWlseTpUdXRvcjstaW5rc2NhcGUtZm9udC1zcGVjaWZpY2F0aW9uOlR1dG9yO2ZpbGw6I2ZmZmZmZjtzdHJva2U6IzAwMDAwMDtzdHJva2Utd2lkdGg6MS45Nzc1MzgyMztzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9rZS1kYXNoYXJyYXk6bm9uZSIgICAgICAgICBpZD0icGF0aDMzODYiIC8+ICAgICAgPHBhdGggICAgICAgICBkPSJtIDQyLjMzNTg4OCw3NS43NzU1NjQgMTEuMDQ1ODMyLDAgMCw3LjgxMzQ3MyAtNS44MTM1OTYsMCAwLDAuNjA0NjE0IDUuODEzNTk2LDAgMCwzLjYwNDQyOSAtMTEuMDQ1ODMyLDAgMCwtMTIuMDIyNTE2IHogbSA1LjIzMjIzNiw0LjIwOTA0MyAwLjU4MTM2LDAgMCwtMC42MDQ2MTQgLTAuNTgxMzYsMCAwLDAuNjA0NjE0IHoiICAgICAgICAgc3R5bGU9ImZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtdmFyaWFudDpub3JtYWw7Zm9udC13ZWlnaHQ6bm9ybWFsO2ZvbnQtc3RyZXRjaDpub3JtYWw7Zm9udC1mYW1pbHk6VHV0b3I7LWlua3NjYXBlLWZvbnQtc3BlY2lmaWNhdGlvbjpUdXRvcjtmaWxsOiNmZmZmZmY7c3Ryb2tlOiMwMDAwMDA7c3Ryb2tlLXdpZHRoOjEuOTc3NTM4MjM7c3Ryb2tlLW1pdGVybGltaXQ6NDtzdHJva2UtZGFzaGFycmF5Om5vbmUiICAgICAgICAgaWQ9InBhdGgzMzg4IiAvPiAgICAgIDxwYXRoICAgICAgICAgZD0ibSA1My45NDQ5MTIsNzUuNzc1NTY0IDUuMjMyMjM2LDAgMCwzLjYwNDQyOSAtNS4yMzIyMzYsMCAwLC0zLjYwNDQyOSB6IG0gNS44MTM1OTYsMCA1LjIzMjIzNiwwIDAsMy42MDQ0MjkgLTUuMjMyMjM2LDAgMCwtMy42MDQ0MjkgeiBtIC01LjgxMzU5Niw4LjQxODA4NyA1LjIzMjIzNiwwIDAsMy42MDQ0MjkgLTUuMjMyMjM2LDAgMCwtMy42MDQ0MjkgeiBtIDUuODEzNTk2LDAgNS4yMzIyMzYsMCAwLDMuNjA0NDI5IC01LjIzMjIzNiwwIDAsLTMuNjA0NDI5IHoiICAgICAgICAgc3R5bGU9ImZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtdmFyaWFudDpub3JtYWw7Zm9udC13ZWlnaHQ6bm9ybWFsO2ZvbnQtc3RyZXRjaDpub3JtYWw7Zm9udC1mYW1pbHk6VHV0b3I7LWlua3NjYXBlLWZvbnQtc3BlY2lmaWNhdGlvbjpUdXRvcjtmaWxsOiNmZmZmZmY7c3Ryb2tlOiMwMDAwMDA7c3Ryb2tlLXdpZHRoOjEuOTc3NTM4MjM7c3Ryb2tlLW1pdGVybGltaXQ6NDtzdHJva2UtZGFzaGFycmF5Om5vbmUiICAgICAgICAgaWQ9InBhdGgzMzkwIiAvPiAgICAgIDxwYXRoICAgICAgICAgZD0ibSA2NS41NTM5MzYsNzEuNTY2NTIgNS4yMzIyMzYsMCAwLDQuMjA5MDQ0IDUuODEzNTk2LDAgMCwzLjYwNDQyOSAtNS44MTM1OTYsMCAwLDQuODEzNjU4IDUuODEzNTk2LDAgMCwzLjYwNDQyOSAtMTEuMDQ1ODMyLDAgMCwtMTYuMjMxNTYgeiIgICAgICAgICBzdHlsZT0iZm9udC1zdHlsZTpub3JtYWw7Zm9udC12YXJpYW50Om5vcm1hbDtmb250LXdlaWdodDpub3JtYWw7Zm9udC1zdHJldGNoOm5vcm1hbDtmb250LWZhbWlseTpUdXRvcjstaW5rc2NhcGUtZm9udC1zcGVjaWZpY2F0aW9uOlR1dG9yO2ZpbGw6I2ZmZmZmZjtzdHJva2U6IzAwMDAwMDtzdHJva2Utd2lkdGg6MS45Nzc1MzgyMztzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9rZS1kYXNoYXJyYXk6bm9uZSIgICAgICAgICBpZD0icGF0aDMzOTIiIC8+ICAgICAgPHBhdGggICAgICAgICBkPSJtIDc3LjE2Mjk2LDc1Ljc3NTU2NCA1LjIzMjIzNiwwIDAsMTIuMDIyNTE2IC01LjIzMjIzNiwwIDAsLTEyLjAyMjUxNiB6IG0gMCwtNC4yMDkwNDQgNS4yMzIyMzYsMCAwLDMuNjA0NDMgLTUuMjMyMjM2LDAgMCwtMy42MDQ0MyB6IiAgICAgICAgIHN0eWxlPSJmb250LXN0eWxlOm5vcm1hbDtmb250LXZhcmlhbnQ6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmb250LXN0cmV0Y2g6bm9ybWFsO2ZvbnQtZmFtaWx5OlR1dG9yOy1pbmtzY2FwZS1mb250LXNwZWNpZmljYXRpb246VHV0b3I7ZmlsbDojZmZmZmZmO3N0cm9rZTojMDAwMDAwO3N0cm9rZS13aWR0aDoxLjk3NzUzODIzO3N0cm9rZS1taXRlcmxpbWl0OjQ7c3Ryb2tlLWRhc2hhcnJheTpub25lIiAgICAgICAgIGlkPSJwYXRoMzM5NCIgLz4gICAgICA8cGF0aCAgICAgICAgIGQ9Im0gODIuOTY3NDcyLDc1Ljc3NTU2NCA1LjgxMzU5NiwwIDAsLTQuMjA5MDQ0IDUuMjMyMjM2LDAgMCwxNi4yMzE1NiAtMTEuMDQ1ODMyLDAgMCwtMTIuMDIyNTE2IHogbSA1LjIzMjIzNiw4LjQxODA4NyAwLjU4MTM2LDAgMCwtNC44MTM2NTggLTAuNTgxMzYsMCAwLDQuODEzNjU4IHoiICAgICAgICAgc3R5bGU9ImZvbnQtc3R5bGU6bm9ybWFsO2ZvbnQtdmFyaWFudDpub3JtYWw7Zm9udC13ZWlnaHQ6bm9ybWFsO2ZvbnQtc3RyZXRjaDpub3JtYWw7Zm9udC1mYW1pbHk6VHV0b3I7LWlua3NjYXBlLWZvbnQtc3BlY2lmaWNhdGlvbjpUdXRvcjtmaWxsOiNmZmZmZmY7c3Ryb2tlOiMwMDAwMDA7c3Ryb2tlLXdpZHRoOjEuOTc3NTM4MjM7c3Ryb2tlLW1pdGVybGltaXQ6NDtzdHJva2UtZGFzaGFycmF5Om5vbmUiICAgICAgICAgaWQ9InBhdGgzMzk2IiAvPiAgICAgIDxwYXRoICAgICAgICAgZD0ibSA5NC41NzY0OTYsNzUuNzc1NTY0IDExLjA0NTgzNCwwIDAsMTIuMDIyNTE2IC0xMS4wNDU4MzQsMCAwLC0xMi4wMjI1MTYgeiBtIDUuMjMyMjM3LDguNDE4MDg3IDAuNTgxMzU3LDAgMCwtNC44MTM2NTggLTAuNTgxMzU3LDAgMCw0LjgxMzY1OCB6IiAgICAgICAgIHN0eWxlPSJmb250LXN0eWxlOm5vcm1hbDtmb250LXZhcmlhbnQ6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmb250LXN0cmV0Y2g6bm9ybWFsO2ZvbnQtZmFtaWx5OlR1dG9yOy1pbmtzY2FwZS1mb250LXNwZWNpZmljYXRpb246VHV0b3I7ZmlsbDojZmZmZmZmO3N0cm9rZTojMDAwMDAwO3N0cm9rZS13aWR0aDoxLjk3NzUzODIzO3N0cm9rZS1taXRlcmxpbWl0OjQ7c3Ryb2tlLWRhc2hhcnJheTpub25lIiAgICAgICAgIGlkPSJwYXRoMzM5OCIgLz4gICAgICA8cGF0aCAgICAgICAgIGQ9Im0gMTA2LjE4NTUyLDcxLjU2NjUyIDUuMjMyMjQsMCAwLDQuMjA5MDQ0IDUuODEzNTksMCAwLDMuNjA0NDI5IC01LjgxMzU5LDAgMCw0LjgxMzY1OCA1LjgxMzU5LDAgMCwzLjYwNDQyOSAtMTEuMDQ1ODMsMCAwLC0xNi4yMzE1NiB6IiAgICAgICAgIHN0eWxlPSJmb250LXN0eWxlOm5vcm1hbDtmb250LXZhcmlhbnQ6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmb250LXN0cmV0Y2g6bm9ybWFsO2ZvbnQtZmFtaWx5OlR1dG9yOy1pbmtzY2FwZS1mb250LXNwZWNpZmljYXRpb246VHV0b3I7ZmlsbDojZmZmZmZmO3N0cm9rZTojMDAwMDAwO3N0cm9rZS13aWR0aDoxLjk3NzUzODIzO3N0cm9rZS1taXRlcmxpbWl0OjQ7c3Ryb2tlLWRhc2hhcnJheTpub25lIiAgICAgICAgIGlkPSJwYXRoMzQwMCIgLz4gICAgICA8cGF0aCAgICAgICAgIGQ9Im0gMTE3Ljc5NDU0LDc1Ljc3NTU2NCAxMS4wNDU4NCwwIDAsNy44MTM0NzMgLTUuODEzNiwwIDAsMC42MDQ2MTQgNS44MTM2LDAgMCwzLjYwNDQyOSAtMTEuMDQ1ODQsMCAwLC0xMi4wMjI1MTYgeiBtIDUuMjMyMjQsNC4yMDkwNDMgMC41ODEzNiwwIDAsLTAuNjA0NjE0IC0wLjU4MTM2LDAgMCwwLjYwNDYxNCB6IiAgICAgICAgIHN0eWxlPSJmb250LXN0eWxlOm5vcm1hbDtmb250LXZhcmlhbnQ6bm9ybWFsO2ZvbnQtd2VpZ2h0Om5vcm1hbDtmb250LXN0cmV0Y2g6bm9ybWFsO2ZvbnQtZmFtaWx5OlR1dG9yOy1pbmtzY2FwZS1mb250LXNwZWNpZmljYXRpb246VHV0b3I7ZmlsbDojZmZmZmZmO3N0cm9rZTojMDAwMDAwO3N0cm9rZS13aWR0aDoxLjk3NzUzODIzO3N0cm9rZS1taXRlcmxpbWl0OjQ7c3Ryb2tlLWRhc2hhcnJheTpub25lIiAgICAgICAgIGlkPSJwYXRoMzQwMiIgLz4gICAgPC9nPiAgPC9nPjwvc3ZnPg==);
}
h2.filename {
  font-family: monospace;
}
h1.textidote {
  width: 378px;
  height: 68px;
  display: block;
}
.keyword1 {
  font-weight: bold;
  color: green;
}
.keyword2 {
  font-weight: bold;
  color: darkblue;
}
.comment, .comment * {
  color: darkred;
  font-weight: normal;
}
.linenb {
  font-style: italic;
  color: lightgrey;
  width: 30pt;
  float: left;
  margin-top: 1pt;
  margin-bottom: 1pt;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.codeline {
  margin-left: -30pt;
  padding-left: 60pt;
  margin-top: 1pt;
  margin-bottom: 1pt;
}
.no-text {
  display: none;
}
.clear {
  clear: both;
}
</style>
</head>
<body>
<a href="https://sylvainhalle.github.io/textidote"><h1 class="textidote"><span class="no-text">Results of TeXtidote analysis</span></h1></a>
<p>Here is the result of analyzing your file(s) with TeXtidote. Hover the mouse over highlighted portions of the document to read a tooltip that gives you some writing advice.</p>
<p>Found 16 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline"><span class="comment"><span class="comment">% Chapter Template</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 89 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span>{Literature Review} <span class="comment">% Main chapter title</span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline"><span class="keyword1">\label</span>{chp:related} <span class="comment">% Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">CNN outperformed traditional computer vision approaches for medical image classification due to its current advances on architecture design. Typically, CNN architectures are composed of two parts~\cite{krizhevsky2012imagenet}. First part is called a convolutional part which further consists of  convolutional layers interconnected with some connection patterns that are responsible for feature extraction. Second part is the fully connected layers which consist of Dense layers that are responsible for classification. This section discusses previous CNN based CXR classification systems  reporting the modeling mechanisms and the reported results.</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline"><span class="keyword1">\section</span>{CNN Based Models}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">In this section CNN base models for covid detection is reviewed.</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline"><span class="keyword1">\subsection</span>{Reliable COVID-19 Detection Using Chest X-ray Images}</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline"><span class="keyword2">\begin{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">    <span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=\textwidth]{Figures/RltDag.png}</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">        <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{rlt:dag} ReCovNet model proposed by~\cite{dag}. It uses pretrained encoder for classification of COVID-19 CXR image</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">    <span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline"><span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite{</span>dag}, authors have adapted transfer learning techniques to train CNN models. Training is performed through two phases. Fig.~\ref{rlt:dag} represent proposed system <span class="highlight-sh" title="Do not use 'by [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">by \cite{</span>dag}. In phase I, u-shaped architecture is excluded by removing the skip connections, which performs the concatenation operation. The reason for constructing an encoder-decoder network without skip connections is that the contributions from the initial layers are avoided; therefore, the network can make decisions from the high level features that are closer to segmentation mapping of the input image.  In phase II, Decoder network is discarded, and the encoder network is fine-tuned for the binary classification task. Adam optimizer is used to train their network. Model is trained with QaTa-COV19 dataset. The method achieved a performance of 98.57\% for sensitivity and 99.77\%  for specificity.</div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline">\<span class="highlight-sh" title="This subsection is very short (about 81 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsection</span>{Advance Warning Methodologies for</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">COVID-19 Using Chest X-Ray Images}</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline"><span class="comment"><span class="comment">% In \cite{ar}, authors have proposed Convolutional Support Estimator Networks (CSENs) classifier. CSEN is used to classify features extracted by DenseNet-121. DenseNet-121 is pre-trained using ImageNet Dataset then fine-tuned using QaTa-Cov19. CSEN achieved 97\% of sensitivity and over 95.5\% of specificity. Authors also evaluated DenseNet-121 achieving 95\% of  sensitivity and 99.74\% of specificity.</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline"><span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite{</span>ar}, authors evaluated the performance of different classifier for classifying the feature produced by DenseNet121. Their methedology is as follows:</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline"><span class="keyword1">\subsubsection</span>{Feature Extraction using DensNet121}</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">They pretrained two DenseNet121~\cite{huang2017densely} models on Early-QaTa-COV19 and ChestX-ray14 datasets are used to extract 1024-D feature vectors by taking the output after global pooling just before the classification layer. Then, a dimensionality reduction is applied over the calculated features with principal component analysis (PCA) by choosing the first $512$ principal components.</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline"><span class="keyword1">\subsubsection</span>{Representation-based classification}</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline"> Representation-based classification (RC) techniques are used in many different classification tasks such as face recognition in~\cite{wright2010sparse}, hyperspectral image classification~\cite{li2016survey}, and human action recognition~\cite{guha2011learning}. Authors classified the feature vector produced by DenseNet121 using either Sparse RC or Collaborative RC. It is performed as follows:</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline"> <span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline">    <span class="keyword2">\begin{split}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">        \hat{x} = \arg \min_{x}(\lambda \vert \vert x \vert\vert_{1} + \vert\vert y - Dx \vert\vert_{2})</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline">    <span class="keyword2">\end{split}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">where $D$ is the dictionary of feature produced by DenseNet121 and projected using PCA. Sparse RC minimizes $\ell^{1}$ of $\hat{x}$. While Collaborative RC minimizes $\ell^{2}$ of $\hat{x}$ as follows:</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">    <span class="keyword2">\begin{split}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">        \hat{x} = \arg \min_{x}(\lambda \vert \vert x \vert\vert_{2} + \vert\vert y - Dx \vert\vert_{2})</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">    <span class="keyword2">\end{split}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">For both types of representations $\hat{x}$ reconstruction error is calculated as follows:</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">    <span class="keyword2">\begin{split}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline">        e_i = \vert\vert y - D\hat{x} \vert\vert_2</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">    <span class="keyword2">\end{split}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline">Then assign a class of the lower construction error as follows:</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">    <span class="keyword2">\begin{split}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">        class(y) \arg \min(e_i)</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">    <span class="keyword2">\end{split}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">Fig.~\ref{fig:RC} represent representation classification pipeline presented in~\cite{ar}. They achieved a $.98$ and $.97$ of accuracy for Sparse Representation-based classification and Collaborative Representation-based classification, respectively. </div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline"><span class="keyword2">\begin{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[width=\textwidth]{Figures/RCpipline.png}</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">    <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{fig:RC} sparse and Collaborative Representation learning pipeline of~\cite{ar}</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">Authors of~\cite{ar} also evaluated SVM for classifying features produces by DenseNet121 and recorded $.98$ of accuarcy.</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword2">\begin{algorithm}</span>[H]</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline"><span class="comment"><span class="comment">%     \DontPrintSemicolon</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline"><span class="comment"><span class="comment">%     <span class="keyword1">\caption</span>{Sparse Representation-based Classification (SRC)}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline"><span class="comment"><span class="comment">%     \KwIn{a matrix of training samples $A = [A_{1}, A_{2}, \dots ,A_{k}] \in \mathbb{R}^{m \times n}$ </span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline"><span class="comment"><span class="comment">%       for $k$ classes, a test sample $\mathbf{y} \in \mathbb{R}^{m}$, (and an optional error tolerance $\varepsilon &gt; 0$).}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline"><span class="comment"><span class="comment">%     Normalize the columns of $A$ to have unit $\ell^{2}$-norm.\;</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline"><span class="comment"><span class="comment">%     Solve the $\ell^{1}$-minimization problem:</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline"><span class="comment"><span class="comment">%     $\hat{\bm{x}}_{1} = \arg \min_{x}\norm{\bm{x}}_{1}\quad \text{subject to}\quad A\bm{x} = \bm{y}$ \;</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline"><span class="comment"><span class="comment">%     (Or alternatively, solve</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline"><span class="comment"><span class="comment">%     $\hat{\bm{x}}_{1} = \arg \min_{x}\norm{\bm{x}}_{1}\quad \text{subject to}\quad \norm{A\bm{x} = \bm{y}}_{2} \leqslant \varepsilon$).\;</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline"><span class="comment"><span class="comment">%     Compute the residuals $r_{i}(\bm{y}) = \norm{\bm{y} - A \delta_{i}(\hat{\bm{x}}_{1})}_{2}$\;</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline"><span class="comment"><span class="comment">%     \For{$i = 1,\dots,k$}{something}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline"><span class="comment"><span class="comment">%   <span class="keyword2">\end{algorithm}</span></span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">  </div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline"><span class="keyword1">\subsection</span>{COVID-19 Detection Using DL Algorithm on CXR Images}</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">In~\cite{akt}, authors proposed a modified MobileNetV2 CNN model where the stander convolution is replaced by a depth-wise convolution. Their model is trained with $3616$ COVID-19 CXR images and 10,192 normal CXR images. Dataset is initially preprocessed by reshaping input images to $299\times299$ and image enhancement technique is applied. The model achieved an accuracy of 98\% for binary classification task. Fig.~\ref{fig:akterPipeline} illustrates general pipeline proposed by~\cite{akt} which is detailed as follows:</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline"><span class="keyword2">\begin{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">    <span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=\textwidth]{Figures/AkterPipeline.png}</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">        <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{fig:akterPipeline} Pipeline proposed in~\cite{akt} for COVID-19 classification</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">    <span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">Preprocessing phase of~\cite{akt} initially performs image augmentations which includes horizontal flip, rotation, width shift and height shift on all the extracted data from the original dataset. Also, image enhancement applied Histogram equalization, Spectrum, Grays and Cyan. The N-CLAHE algorithm was then used to normalize pictures and highlight smaller features for machine learning classifiers to notice. Thereafter, the images were resized to the classifier?s standard resolution. After resizing the picture, the machine learning classifier used the enhanced (52,000) images in a ratio of 80\% data for training, whereas 20\% was used for testing. They proposed modified MobileNetV2 network and achieved 98\% of accuracy</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline"><span class="highlight-sh" title="Do not use 'In [X]': the syntax of a sentence should not be changed by the removal of a citation. [sh:c:noin]">In \cite{</span>hyp} authors have proposed a hybrid model for multilabel classification where VGG16 is used as a feature extractor. Their system is trained by a combined dataset of QaTaCov19 and Chest X-Ray achieving accuracy of $91.09$\%.</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]"><span class="highlight-sh" title="This section is very short (about 1 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="This section is very short (about 2 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span></span></span>{Traditional Computer Vision Models}</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">\<span class="highlight-sh" title="Avoid stacked headings, i.e. consecutive headings without text in between. [sh:stacked]">subsection</span>{ COVID-19 detection in CXR images using</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">majority voting based classifier ensemble}</div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">In~\cite{acos}, authors proposed a two phases COVID-19 multi-label classification system as illustrated in Fig.~\ref{fig:Acos}. The system phase I classifies the input CXR images as normal or COVID-19. In case the image  is classified as COVID-19, phase II further classifies this image as pneumonia or COVID-19. Phase I and Phase II have the same structure with a total of $8196$  local features are extracted then classified using ensemble module. Base classifier of the ensemble module consist of NB, ANN, DT and SVM. A majority voting is used to combine the predictions of these classifiers. Their system achieved accuracy of $98.062$\%  and $91.329$\% for Phase I and Phase II, respectively. Both Phases have same anatomy of preprocessing and feature Description.</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline"><span class="keyword2">\begin{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">    <span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline">        <span class="keyword1">\includegraphics</span>[width=\textwidth]{Figures/ACoS.png}</div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">        <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{<span class="keyword1">\label</span>{fig:Acos} Proposed machine learning pipeline of~\cite{acos}</span>}</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">    <span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">Feature extraction is performed by extracting a total of 8196 features as follows:</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline"><span class="keyword2">\begin{itemize}</span></div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">    <span class="keyword1">\item</span> 8 features using FOSF~\cite{srinivasan2008statistical}.</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">    <span class="keyword1">\item</span> 88 feature using GLCM~\cite{gomez2012analysis}.</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">    <span class="keyword1">\item</span> 8100 feature using HOG~\cite{dalal2005histograms}.</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline"><span class="keyword2">\end{itemize}</span></div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">binary grey wolf optimization (BGWO)~\cite{mirjalili2014grey} is to select the most relavant features from the previously extacted features. The final prediction of the set is the majority vote of seven benchmark classifiers (ANN, KNN, NB, DT, SVM (linear kernel, radial basis function (RBF) kernel, and polynomial kernel).</div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">\input{Tables/relatedlatex.tex}</div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 140 words). You should consider merging it with another section or make it longer. [sh:seclen]"><span class="highlight-sh" title="This section is very short (about 140 words). You should consider merging it with another section or make it longer. [sh:seclen]">section</span></span>{Summary}</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">CNN is a powerful computer vision model. It used intensively for COVID-19 detection. Table~\ref{tbl:related} summarizes recent related work for COVID-19 detection. Despite the high accuracy of current CNN based CXR classification methods, but these methods don't address the problem that CNN is scale variant model. This problem hinders the recognition of large scale COVID-19 pneumonia. In this thesis, a novel scale-invariant CNN architecture is proposed for classification of COVID-19 pneumonia. The proposed architecture deploys Atrous convolution method for learning the scale-invariant features. Then, an attention model is utilized to automatically and internally select at which scale  CNN should consider and ignore other scales. The proposed architecture exploits texture augmentation to reduce the overfitting and artificially enlarge the training dataset. The experimental results show that proposed system outperforms  the previous  CNN based CXR classification methods  with lower trainable parameter number.</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">&nbsp;</div><div class="clear"></div>
</div>
<hr/>
Output produced by TeXtidote v0.8.2, &copy; 2018-2020 Sylvain Hall&eacute; - All rights reserved.<br/>
See the <a href="https://sylvainhalle.github.io/textidote">TeXtidote website</a> for more information.
</body>
</html>

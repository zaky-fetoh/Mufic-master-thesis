<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TeXtidote analysis</title>
<style type="text/css">
body {
  font-family: sans-serif;
}
.highlight, .highlight-sh, .highlight-spelling {
  padding: 2pt;
  border-radius: 4pt;
  cursor: help;
  opacity: 0.7;
  border: dashed 1px;
}
.highlight {
  background-color: orange;
  color: black;
}
.highlight-sh {
  background-color: yellow;
  color: black;
}
.highlight-spelling {
  background-color: red;
  color: white;
}
div.original-file {
  font-family: monospace;
  font-size: 11pt;
  background-color: #f8f8ff;
  padding: 20pt;
  border-radius: 6pt;
}
.textidote {
  	background-image: url(data:image/svg+xml;base64,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);
}
h2.filename {
  font-family: monospace;
}
h1.textidote {
  width: 378px;
  height: 68px;
  display: block;
}
.keyword1 {
  font-weight: bold;
  color: green;
}
.keyword2 {
  font-weight: bold;
  color: darkblue;
}
.comment, .comment * {
  color: darkred;
  font-weight: normal;
}
.linenb {
  font-style: italic;
  color: lightgrey;
  width: 30pt;
  float: left;
  margin-top: 1pt;
  margin-bottom: 1pt;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.codeline {
  margin-left: -30pt;
  padding-left: 60pt;
  margin-top: 1pt;
  margin-bottom: 1pt;
}
.no-text {
  display: none;
}
.clear {
  clear: both;
}
</style>
</head>
<body>
<a href="https://sylvainhalle.github.io/textidote"><h1 class="textidote"><span class="no-text">Results of TeXtidote analysis</span></h1></a>
<p>Here is the result of analyzing your file(s) with TeXtidote. Hover the mouse over highlighted portions of the document to read a tooltip that gives you some writing advice.</p>
<p>Found 4 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline"><span class="comment"><span class="comment">% Chapter Template</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline">\<span class="highlight-sh" title="This section is very short (about 148 words). You should consider merging it with another section or make it longer. [sh:seclen]">chapter</span>{Proposed Methodology I} <span class="comment">% Main chapter title</span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline"><span class="keyword1">\label</span>{chp:proposed1} <span class="comment">% Change X to a consecutive number; for referencing this chapter elsewhere, use~\ref{ChapterX}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline">In this chapter, a novel architecture is introduced for detecting COVID-19 that is designed to be lightweight. The architecture is based on two key components: spatial kernel separability and residual connection. By exploiting spatial kernel separability, the number of training parameters is significantly reduced, making the model more computationally efficient. Additionally, residual connections are used extensively to maintain network stability during the training process and to provide the model with regularization effects that reduce overfitting. This combination of spatial kernel separability and residual connections creates a lightweight architecture that is highly effective at detecting COVID-19. Overall, this chapter provides a valuable contribution to the field of COVID-19 detection by introducing a novel and efficient architecture that can help to identify the disease quickly and accurately. </div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[th]</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline">    \centering</div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[height=40mm,width=8.0cm]{Figures/fig1.jpg}</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline">    <span class="comment">% \decoRule</span></div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline">    <span class="keyword1">\caption</span>{The phases of the proposed method I.}</div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline">    <span class="keyword1">\label</span>{fig1}</div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline">    <span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline"><span class="keyword1">\section</span>{Methodology I}</div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline">In this section, a  proposed  method I to detect COVID-19 disease from chest X-Ray images is presented. The proposed method exploits CNN model to classify the input chest X-Ray image to one of two categories; normal case or Covid-19 case. The proposed  method I consists of three phases: preprocessing, feature extraction, and classification. The proposed method phases are shown in Fig.~\ref{fig1}. </div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline"><span class="keyword1">\subsection</span>{Preprocessing Phase}</div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline">The preprocessing phase is responsible for resizing and normalizing the  input  chest X-Ray images. The pre-processing phase is employed to maintain the numerical stability of the model and reduce the co-variance shift~\cite{lecun1989handwritten}. In addition, this phase leads the learning model of CNN model to reduce  the required overhead to adapt to the different scales of different features of the input data. Reshaping size is determined empirically. The input chest X-Ray image is re-sized and then adapted and normalized to a normal distribution as follows:</div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline">Y := \frac{x_i - \mu_{\mathcal B}}{\sqrt{\sigma_{\mathcal B}^2 + \epsilon}}</div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline"><span class="keyword1">\label</span>{eq1}</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline">where $\mu$ and $\sigma$ is the mean and standard deviation of chest X-Ray image (X), respectively.</div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">After re-sizing the input chest X-Ray image, the input image is normalized to have a zero mean and unit standard deviation. Then,  the image can be scaled and shifted with a normalization parameter which is determined and adapted by the training dataset during the training process according to the following equation: </div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">Z := w_1 Y + w_2</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline"><span class="keyword1">\label</span>{eq2}</div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline">where $w_1$ and $w_2$ are a trainable parameter.</div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">Unlike the  normalization method presented in~\cite{ioffe2015batch}, the batch normalization process presented in this paper has $z$-score normalization parameter that is used in both training and validation phases.</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline"><span class="keyword1">\subsection</span>{Feature Extraction and Classification}</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline">CNN models achieved an outstanding success in image recognition~\cite{lecun2015deep}. This phase  is responsible for extracting spatial features from the normalized chest X-Ray image using a tailored CNN model.  This phase is based on learning the CNN model by the input preprocessed chest X-Ray images. The design of the tailored CNN model is described as follows: </div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline"><span class="keyword1">\subsubsection</span>{Separable CNN kernels}</div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline"><span class="keyword2">\begin{figure*}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline"><span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline"><span class="keyword1">\includegraphics</span>[height=33mm,width=14.0cm]{Figures/fig2.jpg}</div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline"><span class="keyword1">\caption</span>{Separable convolution  $Gy$ and $Gx$ have kernel size of $M\times1$ and $1 \times M$. The combination of these kernels is approximately a $M\times M$ kernel  and depth wise convolution are applied by a $1\times1$ convolution. The output depth  is padded with zeros to have the same spatial size of  $Gy, Gx$. $Gy, Gx$ are performed channel wise. }</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline"><span class="highlight-sh" title="This figure is missing a label [sh:figref]">\</span>label{fig2}<span class="keyword2">\end{center}</span><span class="keyword2">\end{figure*}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">    </div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline">Kernel separability~\cite{rigamonti2013learning}~\cite{szegedy2017inception} is based on decomposing a 2D convolution kernel to linear combinations of two 1D vectors which leads to a large reduction in  the total number of resulting parameters. For example, a 2D kernel of size $9 \times 9$ has a total number of $9^2 = 81$  trained parameters. Whereas in the case of separating this 2D kernel to  linear combinations of two 1D vectors of sizes $9 \times 1$ and $1 \times 9$, this results in a total number of  $9 + 9 = 18$ trained parameters. As a consequence, kernel separability reduces the number of CNN model operations (such as the multiplication and the addition). A  2D kernel of $k \times k$ applied for 2D signal with spatial dimensions of $ M \times N$ has a total number of  $(N-4)(M-4)\times k^2$ operations but in case of  applying kernel separability  yields $2(N-4)(M-4)k$ operations. The flow of separated convolution operations are summarized in Fig.~\ref{fig2}. Fig.~\ref{fig3} represents the structure, denoted by Separated Convolutional Layer, used in the proposed method with kernel size of $(M\times N)$ and satisfying the convolutional kernel separability. Separated Convolutional Layer is composed of three consecutive layers. The first convolutional layer has a kernel size of $(M\times1)$ and the number of convolutional neuron and  filters are equal to the number of channels as the input feature map and the convolution operations are performed in a channel wise. The second layer  operates in the same way as the first layer but it has a kernel of size $(1\times M)$. The third layer is the convolutional layer with kernel of size $(1\times1)$ and number of convolutional neuron is $N$. The collaboration of the three layers are  connected to preform similarly to the convolutional layer with kernel size of $(M\times M)$ and number of neuron and filter are the same as $N$ but with large difference in the performance.</div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline"><span class="keyword2">\begin{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">    <span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[height=34mm,width=7.0cm]{Figures/fig3.jpg}</div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">    <span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{Separated Convolutional Layer</span>}{ composed of three consecutive layers. The first Convolutional layer has a kernel size of $(M\times1)$ and $D$  convolutional neuron. The second layer  operates in the same way as the first layer but it has a kernel of size $(1\times M)$ and $D$ convolutional neuron. The third layer is the convolutional layer with kernel of size $(1\times1)$ and number of convolutional neuron is $N$.}</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline">    <span class="keyword1">\label</span>{fig3}</div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">    <span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">    <span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline"><span class="keyword1">\subsubsection</span>{ Batch Normalization and  Activation function}</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline">In the proposed method linear separable convolutional kernels are followed by a batch normalization and an activation function. Rectified Linear Unit (ReLU)~\cite{he2015delving} is a nonlinear activation that allows the network to fit and approximate highly non-linear datasets distribution. The proposed method employs the batch normalization which is described in~\cite{ioffe2015batch}. </div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">Batch Normalization~\cite{ioffe2015batch} reduces internal covariate shift produced as a result of  moving between layers during the feedforward procedure~\cite{ioffe2015batch}. Batch Normalization makes the loss landscape smoother and reduces the number of saddle points~\cite{santurkar2018does} which allows to use higher learning rates. Using a higher learning rate makes the network training  faster~\cite{ioffe2015batch}. Batch normalization reduces the vanishing gradient problem and exploding gradient problem as it makes the resulted activation scale independent from the trainable parameter scale~\cite{ioffe2015batch}. Batch normalization has the effect of regularization because of the inherited randomness when selecting the batch sample~\cite{ioffe2015batch} which help the generalization to unseen chest X-Ray image.</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline"><span class="keyword1">\subsubsection</span>{ Deep and larger receptive field Network design}</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">Deeper convolutional neural network design is a very important task for any image recognition task~\cite{he2016deep}. Training a deeper network is very expensive and has many challenges such as vanishing gradient problem, exploding gradient problem, and degradation problem~\cite{he2016deep}. Exploding gradient problem occurs  when the  gradient update becomes very large (approaching infinity) resulting in the network diversion. Vanishing gradient problem occurs when the  gradient update becomes very small (approaching zero) resulting in preventing the parameter update for early layers~\cite{ioffe2015batch} and preventing the network to learn new patterns. Batch normalization~\cite{ioffe2015batch} and the use of ReLU activation function~\cite{krizhevsky2012imagenet} alleviate these two problems.</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">The deep layers of CNN networks sometimes need to  approximate the identity function which is not a simple task especially  with the existence of a non-linear functions. Residual connection~\cite{he2016deep} overcomes this problem by using skip connection as shown in Fig.~\ref{fig4}.</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">Fig.~\ref{fig4} represents the building block layer of the feature extraction phase, denoted by stack of Residual Separated Block  (RSB). RSB consists of four layers of separated convolutional layers, each layer is followed by a batch normalization and an activation function. It has an output of depth $N$ where each sublayer produces an output of depth $N/4$ which is concatenated at the end of the layer to produce a depth  $N$. RSB produces a feature map that includes both low level features and high level features.</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline"><span class="keyword2">\begin{figure*}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline"><span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline"><span class="keyword1">\includegraphics</span>[height=38mm,width=14.0cm]{Figures/fig4.jpg}</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline"><span class="keyword1">\caption</span>{The stack of residual separated block  (RSB) consists of four layer of separated convolutional layer each of which is followed by batch normalization and activation function.}</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline"><span class="keyword1">\label</span>{fig4}</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline"><span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline"><span class="keyword2">\end{figure*}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline"><span class="keyword2">\begin{figure*}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">    <span class="keyword2">\begin{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">    <span class="keyword1">\includegraphics</span>[height=37mm,width=14.0cm]{Figures/fig5.jpg}</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">    <span class="keyword1">\caption</span>{The complete proposed tailored CNN architecture.}</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">    <span class="keyword1">\label</span>{fig5}</div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">    <span class="keyword2">\end{center}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">    <span class="keyword2">\end{figure*}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline">    </div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline">Unlike the traditional neural network, which is fully connected to the previous layer, convolutional neural network is connected locally to a local region of the previous feature map. This introduces the concept of the network receptive field~\cite{luo2016understanding}. Receptive field should be large enough to capture large patterns in the input chest X-Ray image. Therefore, any consecutive convolutional layers in the proposed method without a pooling layer in between a larger kernel size is used in one of them. Residual Separated block, RSB, in Fig.~\ref{fig4} may have kernel sizes of 3, 5, 7, and 9, respectively.<span class="highlight-sh" title="You should not break lines manually in a paragraph. Either start a new paragraph or stay in the current one. [sh:nobreak]">\</span>\</div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">Fig.~\ref{fig5} Represent a complete CNN architecture.</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline"><span class="keyword1">\section</span>{Summary}</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline"><span class="comment"><span class="comment">% In this chapter a lightweight CNN architecture is proposed for COVID19 detection. Proposed architecture is based on spatial separability of the convolutional kernel to enforce the learning of linear kernels. The proposed architecture consists of separated kernels convolutional layers that is connected by a residual connection. The proposed architecture uses batch normalization to maintain the network stability during the training process.</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline"><span class="comment"><span class="comment">% In this chapter, a novel lightweight CNN architecture is proposed for COVID-19 detection that is based on the concept of spatial kernel separability. The proposed architecture is designed to reduce the number of training parameters and improve computational efficiency by exploiting the separability of convolutional kernels. By learning linear kernels, the model can perform faster and more accurately on the given dataset. The proposed architecture includes separated kernel convolutional layers that are connected by a residual connection, which helps maintain network stability during training. Additionally, the proposed architecture utilizes batch normalization, a technique that helps to standardize the inputs of each layer and improve the convergence rate of the model. By combining these techniques, the proposed architecture offers a lightweight, efficient, and accurate method for detecting COVID-19.</span></span></div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline">In this chapter, the proposed lightweight CNN architecture for COVID-19 detection is designed with the concept of spatial separability in mind. The spatial separability of the convolutional kernel is used to enforce the learning of linear kernels, which reduces the number of training parameters and improves computational efficiency. Essentially, the model is designed to recognize patterns in the data that are linearly separable, which enables the use of simpler and more efficient models.</div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline">The proposed architecture comprises of separated kernel convolutional layers that are connected by a residual connection. The use of separated kernel convolutional layers helps to reduce the number of training parameters, while the residual connection improves network stability during the training process. The residual connection enables the model to retain important features while also discarding unimportant ones, which helps prevent overfitting.</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">Batch normalization is used in the proposed architecture to maintain network stability during the training process. The technique standardizes the inputs of each layer, which improves the convergence rate of the model. This is done by normalizing the layer inputs to have zero mean and unit variance, which helps prevent internal covariate shift. Internal covariate shift refers to the change in the distribution of network activations that occurs during training, which can slow down the convergence rate.</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">&nbsp;</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">In summary, the proposed lightweight CNN architecture for COVID-19 detection is based on the spatial separability of the convolutional kernel, with separated kernel convolutional layers connected by a residual connection. Batch normalization is used to maintain network stability during training, which improves the convergence rate of the model. By combining these techniques, the proposed architecture offers an efficient, accurate, and stable method for detecting COVID-19.</div><div class="clear"></div>
</div>
<hr/>
Output produced by TeXtidote v0.8.2, &copy; 2018-2020 Sylvain Hall&eacute; - All rights reserved.<br/>
See the <a href="https://sylvainhalle.github.io/textidote">TeXtidote website</a> for more information.
</body>
</html>

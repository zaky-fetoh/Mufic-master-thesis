\relax 
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{7}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chp:background}{{2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convolutional Neural Networks (CNN)}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Typical CNN architecture\relax }}{7}{}\protected@file@percent }
\newlabel{lenet}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Convolutional Layer}{8}{}\protected@file@percent }
\newlabel{eq:logistic}{{2.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Convolutional layer with single input feature map and four convolutional kernels\relax }}{9}{}\protected@file@percent }
\newlabel{conv2Dlayer}{{2.2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Activation functions}{9}{}\protected@file@percent }
\newlabel{sec:activations}{{2.2}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Sigmoid}{9}{}\protected@file@percent }
\newlabel{sec:logistic}{{2.2.1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Some of the most common activation functions: sigmoid, tanh, ReLU, and Leaky ReLU. ReLU and Leaky ReLU are overlapping for $z \geq 0$.\relax }}{10}{}\protected@file@percent }
\newlabel{fig:activations}{{2.3}{10}}
\newlabel{eq:logistic}{{2.2}{10}}
\newlabel{eq:logistic_derivative}{{2.3}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Hyperbolic tangent (tanh)}{11}{}\protected@file@percent }
\newlabel{sec:tanh}{{2.2.2}{11}}
\newlabel{eq:tanh}{{2.4}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Rectified Linear Unit (ReLU)}{11}{}\protected@file@percent }
\newlabel{sec:relu}{{2.2.3}{11}}
\newlabel{eq:relu}{{2.5}{11}}
\newlabel{eq:relu_derivative}{{2.6}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Leaky Rectified Linear Unit (Leaky ReLU)}{12}{}\protected@file@percent }
\newlabel{sec:lrelu}{{2.2.4}{12}}
\newlabel{eq:lrelu}{{2.7}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Softmax}{12}{}\protected@file@percent }
\newlabel{sec:softmax}{{2.2.5}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The steepness of softmax function as temperature $T$ grows.\relax }}{13}{}\protected@file@percent }
\newlabel{fig:softmax}{{2.4}{13}}
\newlabel{eq:softmax}{{2.8}{13}}
\newlabel{eq:softmax_tmp}{{2.9}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Pooling}{13}{}\protected@file@percent }
\newlabel{sec:pooling}{{2.3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Computing the output values of a $3 \times 3$ average pooling and Max Pooling operation on a $5 \times 5$ input using $1 \times 1$ strides.\relax }}{14}{}\protected@file@percent }
\newlabel{fig:numerical_avg_max_pooling}{{2.5}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Average Pooling}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Max pooling}{14}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  spatial pyramid pooling layer. Input feature map is divided into pins for each pin an aggregation function is performed\relax }}{15}{}\protected@file@percent }
\newlabel{originalSpp}{{2.6}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Spatial Pyramid Pooling}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Channel attention block of SE network\relax }}{16}{}\protected@file@percent }
\newlabel{fig:SE_block}{{2.7}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Neural Network Attention}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Channel Attention}{16}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces  Spatial attention mechanism \relax }}{17}{}\protected@file@percent }
\newlabel{fig:spattex}{{2.8}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Spatial Attention}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Normalization}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Batch Normalization}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Layer Normalization}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Augmentation}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Dropout}{19}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Data augmentation using crop augmentation with different preserving degrees and the corresponding accuracy of recognizing certain classes using ResNet \blx@tocontentsinit {0}\cite {balestriero2022effects}\relax }}{20}{}\protected@file@percent }
\newlabel{fig:DADeg}{{2.9}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces  Dropout Neural network Model. \textbf  {Left}: A standard network with two hidden layers. \textbf  {Right}: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped \blx@tocontentsinit {0}\cite {srivastava2014dropout}.\relax }}{20}{}\protected@file@percent }
\newlabel{fig:Dropout}{{2.10}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Multiscale Recognition}{21}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces A grid represents a feature map and the circle inside the cell represents the corresponding value of $2\times 2$ convolutional kernel. \textbf  {left}: A $2\times 2$ convolutional kernel with an atrous rate $r=1$. \textbf  {middle}: A $2\times 2$ convolutional kernel with an atrous rate $r=2$. \textbf  {right}: A $2\times 2$ convolutional kernel with an atrous rate $r=3$.\relax }}{22}{}\protected@file@percent }
\newlabel{AtrousConv}{{2.11}{22}}
\@setckpt{Chapters/Chapter2_Backgr}{
\setcounter{page}{23}
\setcounter{equation}{13}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{9}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{0}
\setcounter{LT@tables}{2}
\setcounter{LT@chunks}{1}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{96}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{blx@maxsegment@0}{0}
\setcounter{blx@sectionciteorder@0}{96}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{parentequation}{0}
\setcounter{footdir@label}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
}

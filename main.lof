\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Samples of CXR images of Normal and COVID-19 pneumonia\relax }}{2}{}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Typical hospitals routine of reducing COVID-19 spread\relax }}{3}{}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Solution Methodologies proposed in this thesis\relax }}{4}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Typical CNN architecture\relax }}{7}{}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Convolutional layer with single input feature map and four convolutional kernels\relax }}{9}{}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Some of the most common activation functions: sigmoid, tanh, ReLU, and Leaky ReLU. ReLU and Leaky ReLU are overlapping for $z \geq 0$.\relax }}{10}{}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces The steepness of softmax function as temperature $T$ grows.\relax }}{13}{}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Computing the output values of a $3 \times 3$ average pooling and Max Pooling operation on a $5 \times 5$ input using $1 \times 1$ strides.\relax }}{14}{}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces spatial pyramid pooling layer. Input feature map is divided into pins for each pin an aggregation function is performed\relax }}{15}{}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Channel attention block of SE network\relax }}{16}{}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Spatial attention mechanism \relax }}{17}{}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Data augmentation using crop augmentation with different preserving degrees and the corresponding accuracy of recognizing certain classes using ResNet \blx@tocontentsinit {0}\cite {balestriero2022effects}\relax }}{20}{}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Dropout Neural network Model. \textbf {Left}: A standard network with two hidden layers. \textbf {Right}: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped \blx@tocontentsinit {0}\cite {srivastava2014dropout}.\relax }}{20}{}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces A grid represents a feature map and the circle inside the cell represents the corresponding value of $2\times 2$ convolutional kernel. \textbf {left}: A $2\times 2$ convolutional kernel with an atrous rate $r=1$. \textbf {middle}: A $2\times 2$ convolutional kernel with an atrous rate $r=2$. \textbf {right}: A $2\times 2$ convolutional kernel with an atrous rate $r=3$.\relax }}{22}{}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces ReCovNet model proposed by~\blx@tocontentsinit {0}\cite {dag}. It uses a pre-trained encoder for the classification of COVID-19 CXR image\relax }}{23}{}%
\contentsline {figure}{\numberline {2.13}{\ignorespaces sparse and Collaborative Representation learning pipeline of~\blx@tocontentsinit {0}\cite {ar}\relax }}{25}{}%
\contentsline {figure}{\numberline {2.14}{\ignorespaces Pipeline proposed in~\blx@tocontentsinit {0}\cite {akt} for COVID-19 classification\relax }}{26}{}%
\contentsline {figure}{\numberline {2.15}{\ignorespaces Proposed machine learning pipeline of~\blx@tocontentsinit {0}\cite {acos}\relax }}{27}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The phases of the proposed method I.\relax }}{31}{}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Separable convolution $Gy$ and $Gx$ have kernel size of $M\times 1$ and $1 \times M$. The combination of these kernels is approximately a $M\times M$ kernel and depth-wise convolution is applied by a $1\times 1$ convolution. The output depth is padded with zeros to have the same spatial size of $Gy, Gx$. $Gy, Gx$ are performed channel-wise. \relax }}{33}{}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Separated Convolutional Layer\relax }}{34}{}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces The stack of residual separated block (RSB) consists of four layers of separated convolutional layer each of which is followed by batch normalization and activation function. Feature maps are concatenated at the end of the block.\relax }}{35}{}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces The complete proposed tailored CNN architecture.\relax }}{35}{}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Proposed method for COVID-19 classification from CXR images.\relax }}{37}{}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Texture Augmentation module\relax }}{38}{}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Texture Augmentation\relax }}{38}{}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Spatially weighted atrous spatial Pyramid Pooling (SWASPP) interal layers within dashed square are parameter shared.\relax }}{39}{}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Attantion module structure used by SWASPP micro-architecture\relax }}{39}{}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces Densely connected SWASPP (DSWASPP): is a stack of densely connected SWASPP, such that the output of any SWASPP is Concatenated to the input of all next layers. All the three layers produce an output of dimension of $C_{in} \times H \times W$.\relax }}{41}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Pneumonia Scales of QaTa-COV19-v1, Y-axis represents the frequency, number of occurrences, of pneumonia with a particular area\relax }}{45}{}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces (a) The training loss and the validation loss of each epoch and (b) The training accuracy and the validation accuracy of each epoch.\relax }}{50}{}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Training profiles of both SPP-net variants and the proposed network. For the same color solid line represents training statistics while the dashed line represents the validation statistics for the corresponding model. \textbf {left:} is the training accuracy. \textbf {Right:} is the training loss.\relax }}{51}{}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces SASPP baseline architecture loss during both training, solid line, and validation, bashed line, compared with Proposed network and best performing SPP architecture.\relax }}{54}{}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Cross entropy loss of the proposed architecture.\relax }}{54}{}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Training and validation accuracy of the proposed architecture.\relax }}{55}{}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces precision-recall trade-off of the proposed network.\relax }}{55}{}%
\addvspace {10\p@ }
